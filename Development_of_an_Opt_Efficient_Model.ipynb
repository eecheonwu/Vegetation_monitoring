{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36c860c7-ff96-41e2-9d98-f25e5701da02",
   "metadata": {
    "id": "36c860c7-ff96-41e2-9d98-f25e5701da02"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torchvision\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.quantization\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "OrtbWLVt4hld",
   "metadata": {
    "id": "OrtbWLVt4hld"
   },
   "outputs": [],
   "source": [
    "seed = 42  # Choose your desired seed value\n",
    "random.seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "else:\n",
    "  torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "CC5N-EbGFDv1",
   "metadata": {
    "id": "CC5N-EbGFDv1"
   },
   "outputs": [],
   "source": [
    "def batch_mean_and_sd(loader):\n",
    "\n",
    "    cnt = 0\n",
    "    fst_moment = torch.empty(3)\n",
    "    snd_moment = torch.empty(3)\n",
    "\n",
    "    for images, _ in loader:\n",
    "        b, c, h, w = images.shape\n",
    "        nb_pixels = b * h * w\n",
    "        sum_ = torch.sum(images, dim=[0, 2, 3])\n",
    "        sum_of_square = torch.sum(images ** 2,\n",
    "                                  dim=[0, 2, 3])\n",
    "        fst_moment = (cnt * fst_moment + sum_) / (cnt + nb_pixels)\n",
    "        snd_moment = (cnt * snd_moment + sum_of_square) / (cnt + nb_pixels)\n",
    "        cnt += nb_pixels\n",
    "\n",
    "    mean, std = fst_moment, torch.sqrt(snd_moment - fst_moment ** 2)\n",
    "    return mean,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ioq87tp8FJje",
   "metadata": {
    "id": "Ioq87tp8FJje"
   },
   "outputs": [],
   "source": [
    "mean, std = batch_mean_and_sd(image_dataloader)\n",
    "print(\"mean and std: \\n\", mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "LrOpFaPE715Y",
   "metadata": {
    "id": "LrOpFaPE715Y"
   },
   "outputs": [],
   "source": [
    "class MinMaxScaler(nn.Module):\n",
    "  def __init__(self, min_values, max_values):\n",
    "    self.min_values = min_values\n",
    "    self.max_values = max_values\n",
    "\n",
    "  def __call__(self, data):\n",
    "    return (data - self.min_values) / (self.max_values - self.min_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7af1b011-fa65-4f24-a5ab-0d82337edeb2",
   "metadata": {
    "id": "7af1b011-fa65-4f24-a5ab-0d82337edeb2"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(600),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.2498, 0.3010, 0.1964), (0.1668, 0.1603, 0.1697)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    MinMaxScaler(1.8, 7.1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a119db1-f7b3-4b67-95e4-4a1436fc0dd0",
   "metadata": {
    "id": "0a119db1-f7b3-4b67-95e4-4a1436fc0dd0"
   },
   "outputs": [],
   "source": [
    "# Define the custom dataset class\n",
    "class ImageLabelDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_file, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_file = label_file\n",
    "        self.transform = transform\n",
    "\n",
    "        # Read labels from CSV file\n",
    "        label_df = pd.read_csv(label_file)\n",
    "        self.image_paths = label_df['image'].tolist()\n",
    "        self.labels = label_df['FVC'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = os.path.join(self.image_dir, self.image_paths[index])\n",
    "        image = Image.open(image_path)\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0778bf9-e5bc-4e04-8d83-6498da93f8df",
   "metadata": {
    "id": "b0778bf9-e5bc-4e04-8d83-6498da93f8df"
   },
   "outputs": [],
   "source": [
    "train_dataset = ImageLabelDataset('/home/emmanuel/Project/ImageDataset/Training', '/home/emmanuel/Project/ImageDataset/Training/training.csv', transform=transform)\n",
    "val_dataset = ImageLabelDataset('/home/emmanuel/Project/ImageDataset/Validation', '/home/emmanuel/Project/ImageDataset/Validation/validation.csv', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f010d57-cdf5-4aaa-999c-5ab27ea2e2d5",
   "metadata": {
    "id": "6f010d57-cdf5-4aaa-999c-5ab27ea2e2d5"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "E508uqmIjV6G",
   "metadata": {
    "id": "E508uqmIjV6G"
   },
   "outputs": [],
   "source": [
    "class EfficientVisionTransformer(nn.Module):\n",
    "\n",
    "  def __init__(self, d_model=600, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.2, batch_first=True):\n",
    "\n",
    "\n",
    "    super().__init__()\n",
    "    # Patch embedding\n",
    "    self.patch_embedding = nn.Conv2d(3, d_model, kernel_size=16, stride=16)\n",
    "\n",
    "    # Efficient Transformer encoder\n",
    "    encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "    self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "\n",
    "    # Decoder\n",
    "    self.decoder = nn.Linear(d_model, 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.patch_embedding(x)\n",
    "    x = x.flatten(2).transpose(1, 2)\n",
    "    x = model.transformer_encoder(x)\n",
    "    x = self.decoder(x.mean(dim=1))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "NR8E8d-KkfUY",
   "metadata": {
    "id": "NR8E8d-KkfUY"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = torch.compile(EfficientVisionTransformer()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3jZaUbn_x_II",
   "metadata": {
    "id": "3jZaUbn_x_II"
   },
   "outputs": [],
   "source": [
    "# Define the optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cn_sB7dCcUQ9",
   "metadata": {
    "id": "cn_sB7dCcUQ9"
   },
   "source": [
    "# Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Q3ExpT7g9Lv-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3ExpT7g9Lv-",
    "outputId": "64b1c94c-f97c-4308-bb8f-e4f13c68ce85"
   },
   "outputs": [],
   "source": [
    "#Trains and Optimize a PyTorch model for a specified number of epochs, tracking loss and accuracy on both training and validation sets.\n",
    "\n",
    "# Define pruning and quantization schedule\n",
    "prune_quantize_schedule = [\n",
    "    (0, 0),  # No pruning at the beginning\n",
    "    (3, 0.4),  # Prune 40% after 2 epochs\n",
    "\n",
    "]\n",
    "\n",
    "train_losses, val_losses = [], []  # Track losses\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "  # Training phase\n",
    "  model.train()  # Set model to training mode\n",
    "  running_loss = 0.0 # Initialize counters\n",
    "\n",
    "  for images, labels in train_dataloader:\n",
    "    images = images.float().to(device)  # Move images to GPU if available\n",
    "    labels = labels.float().to(device) # Move labels to GPU if available\n",
    "\n",
    "\n",
    "    outputs = model(images)  # Forward pass\n",
    "    labels = labels.view(-1, 1)\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "    optimizer.zero_grad()  # Clear gradients\n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Update model parameters\n",
    "\n",
    "\n",
    "    # Update counters\n",
    "    running_loss += loss.item() * images.size(0)\n",
    "\n",
    "  epoch_train_loss = running_loss / len(train_dataloader.dataset)\n",
    "  train_losses.append(epoch_train_loss)\n",
    "\n",
    "\n",
    "  # Validation phase\n",
    "  model.eval()  # Set model to evaluation mode\n",
    "  running_loss, running_corrects = 0.0, 0\n",
    "\n",
    "  with torch.no_grad():  # Disable gradient calculation during validation\n",
    "    for images, labels in val_dataloader:\n",
    "      images = images.float().to(device)\n",
    "      labels = labels.float().to(device)\n",
    "\n",
    "      outputs = model(images)\n",
    "      labels = labels.view(-1, 1)\n",
    "      loss = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "      running_loss += loss.item() * images.size(0)\n",
    "\n",
    "\n",
    "  epoch_val_loss = running_loss / len(val_dataloader.dataset)\n",
    "  val_losses.append(epoch_val_loss)\n",
    "\n",
    "  # Print epoch results\n",
    "  print(\"=============================\")\n",
    "  print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "  print(f\"Train Loss: {epoch_train_loss:.3f}\")\n",
    "  print(f\"Val Loss: {epoch_val_loss:.3f}\")\n",
    "\n",
    "  for prune_epoch, prune_ratio in prune_quantize_schedule:\n",
    "    if epoch == prune_epoch:\n",
    "      for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "          prune.l1_unstructured(module, name='weight', amount=prune_ratio)  # Apply pruning\n",
    "\n",
    "      for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "          torch.quantization.quantize_dynamic(module, {nn.Conv2d}, dtype=torch.qint8)# Apply quantization\n",
    "\n",
    "      print(\"=================================================\")\n",
    "      print(f\"Pruning model by {prune_ratio:.2f} at epoch {epoch + 1}\")\n",
    "      print(f\"Quantizing model at epoch {epoch + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfdea78-174c-4b0c-83ab-1a02bbc6f4fc",
   "metadata": {},
   "source": [
    "# Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f501fed9-b576-4d62-bbc5-3524a8c7c8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientVisionTransformer()  # Instantiate the model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccae9866-3716-4e2c-8cc0-6e3fa17b2c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedModule(\n",
      "  (_orig_mod): EfficientVisionTransformer(\n",
      "    (patch_embedding): Conv2d(3, 600, kernel_size=(16, 16), stride=(16, 16))\n",
      "    (transformer_encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-5): 6 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=600, out_features=600, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=600, out_features=2048, bias=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (linear2): Linear(in_features=2048, out_features=600, bias=True)\n",
      "          (norm1): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((600,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.2, inplace=False)\n",
      "          (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): Linear(in_features=600, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1HUcVOplxBwM",
   "metadata": {
    "id": "1HUcVOplxBwM"
   },
   "source": [
    "# Compute Model FLOPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "emaZNW1oItbO",
   "metadata": {
    "id": "emaZNW1oItbO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                                            Param #\n",
       "==========================================================================================\n",
       "EfficientVisionTransformer                                        --\n",
       "├─Conv2d: 1-1                                                     461,400\n",
       "├─TransformerEncoder: 1-2                                         --\n",
       "│    └─ModuleList: 2-1                                            --\n",
       "│    │    └─TransformerEncoderLayer: 3-1                          2,778,008\n",
       "│    │    └─TransformerEncoderLayer: 3-2                          2,778,008\n",
       "│    │    └─TransformerEncoderLayer: 3-3                          2,778,008\n",
       "│    │    └─TransformerEncoderLayer: 3-4                          2,778,008\n",
       "│    │    └─TransformerEncoderLayer: 3-5                          2,778,008\n",
       "│    │    └─TransformerEncoderLayer: 3-6                          2,778,008\n",
       "├─Linear: 1-3                                                     361\n",
       "==========================================================================================\n",
       "Total params: 17,129,809\n",
       "Trainable params: 17,129,809\n",
       "Non-trainable params: 0\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Assuming your model is named 'model'\n",
    "model = EfficientVisionTransformer()\n",
    "model = torch.load('/home/emmanuel/Project/model3.pt', map_location=torch.device('cpu'))\n",
    "# Load your model here\n",
    "# Get model summary, including FLOPs\n",
    "summary(model, input_tensor=(1,3,600,600))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Tjilfhi8PgPi",
   "metadata": {
    "id": "Tjilfhi8PgPi"
   },
   "source": [
    "# **System Testing**\n",
    "### **Functionality Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sd-JnCmNPVTG",
   "metadata": {
    "id": "sd-JnCmNPVTG"
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "channels = 3\n",
    "height = 600\n",
    "width = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rGVzsa1PPVfY",
   "metadata": {
    "id": "rGVzsa1PPVfY"
   },
   "outputs": [],
   "source": [
    "x = torch.randn(batch_size, channels, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wX_J6MqrPVhx",
   "metadata": {
    "id": "wX_J6MqrPVhx"
   },
   "outputs": [],
   "source": [
    "def test_patch_embedding_output_shape(sample_data):\n",
    "    model = EfficientVisionTransformer()\n",
    "    output = model.patch_embedding(sample_data)\n",
    "    assert output.shape == (batch_size, height, height // 16, width // 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sG0ZQRM6PVkw",
   "metadata": {
    "id": "sG0ZQRM6PVkw"
   },
   "outputs": [],
   "source": [
    "def test_decoder_output_shape(sample_data):\n",
    "    model = EfficientVisionTransformer()\n",
    "    output = model(x)\n",
    "    assert output.shape == (batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "io1SG4EkPVnS",
   "metadata": {
    "id": "io1SG4EkPVnS"
   },
   "outputs": [],
   "source": [
    "def test_transformer_encoder(sample_data):\n",
    "    x = model.patch_embedding(sample_data)\n",
    "    x = x.flatten(2).transpose(1, 2)\n",
    "    output = model.transformer_encoder(x)\n",
    "    assert output.shape == (batch_size, ((height // 16) * (width // 16)), height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vy7V897fRZJK",
   "metadata": {
    "id": "Vy7V897fRZJK"
   },
   "source": [
    "## Performance Testing\n",
    " ### Execution Time on a CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "m6PXfoMuTedz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m6PXfoMuTedz",
    "outputId": "f3cb9f02-4b4b-4d8f-9919-2148b8ee9321"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-01-03 09:50:13 5071:5071 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                             model_inference         3.05%      18.514ms       100.00%     606.853ms     606.853ms             1  \n",
      "                                aten::linear         0.07%     412.000us        43.03%     261.134ms      10.445ms            25  \n",
      "          aten::scaled_dot_product_attention         0.00%      22.000us        42.31%     256.740ms      42.790ms             6  \n",
      "    aten::_scaled_dot_product_attention_math         3.19%      19.362ms        42.30%     256.718ms      42.786ms             6  \n",
      "                                 aten::addmm        37.10%     225.170ms        40.11%     243.401ms      10.142ms            24  \n",
      "                                aten::matmul         0.06%     354.000us        29.33%     178.020ms      13.694ms            13  \n",
      "                                   aten::bmm        26.53%     161.010ms        26.53%     161.010ms      13.418ms            12  \n",
      "                               aten::softmax         0.01%      34.000us        10.96%      66.490ms      11.082ms             6  \n",
      "                              aten::_softmax        10.95%      66.456ms        10.95%      66.456ms      11.076ms             6  \n",
      "                                 aten::copy_         5.75%      34.894ms         5.75%      34.894ms     697.880us            50  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 606.853ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-01-03 09:50:14 5071:5071 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2024-01-03 09:50:14 5071:5071 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "model = EfficientVisionTransformer()\n",
    "model = torch.load('/home/emmanuel/Project/model3.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "inputs = torch.randn(1, 3, 600, 600).to(device)\n",
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        model(inputs)\n",
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gnIOPsbXqqVg",
   "metadata": {
    "id": "gnIOPsbXqqVg"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3nAMHt8q_Jb",
   "metadata": {
    "id": "b3nAMHt8q_Jb"
   },
   "source": [
    "## Memory Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "OpnkTmJ6pjAG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpnkTmJ6pjAG",
    "outputId": "ded02e63-0241-4c92-e544-933e1c2cd9bb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-01-03 09:50:50 5071:5071 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem    # of Calls  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                   aten::bmm        26.17%     163.707ms        26.17%     163.707ms      13.642ms     361.97 Mb     361.97 Mb            12  \n",
      "                              aten::_softmax        10.67%      66.764ms        10.67%      66.764ms      11.127ms     343.17 Mb     343.17 Mb             6  \n",
      "                                 aten::addmm        36.47%     228.187ms        39.56%     247.488ms      10.312ms     148.77 Mb     148.77 Mb            24  \n",
      "                                 aten::empty         0.07%     428.000us         0.07%     428.000us       8.392us     119.16 Mb     119.16 Mb            51  \n",
      "                             aten::clamp_min         1.98%      12.373ms         1.98%      12.373ms       2.062ms      64.17 Mb      64.17 Mb             6  \n",
      "                                   aten::mul         2.64%      16.499ms         2.64%      16.499ms       1.269ms      56.25 Mb      56.25 Mb            13  \n",
      "                                   aten::add         1.62%      10.130ms         1.62%      10.130ms     844.167us      37.60 Mb      37.60 Mb            12  \n",
      "                                   aten::div         1.32%       8.267ms         1.34%       8.414ms     701.167us      37.60 Mb      37.60 Mb            12  \n",
      "                                    aten::mm         3.19%      19.973ms         3.19%      19.973ms      19.973ms       9.40 Mb       9.40 Mb             1  \n",
      "                            aten::layer_norm         0.11%     685.000us         1.68%      10.532ms     877.667us      37.73 Mb       3.14 Mb            12  \n",
      "--------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 625.627ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-01-03 09:50:51 5071:5071 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2024-01-03 09:50:51 5071:5071 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "model = EfficientVisionTransformer()\n",
    "model = torch.load('/home/emmanuel/Project/model3.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "inputs = torch.randn(1, 3, 600, 600).to(device)\n",
    "with profile(activities=[ProfilerActivity.CPU],profile_memory=True, record_shapes=True) as prof:\n",
    "  with record_function(\"model_inference\"):\n",
    "    model(inputs)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vapwmc1IuAfz",
   "metadata": {
    "id": "vapwmc1IuAfz"
   },
   "source": [
    "## Size (Computational and Memory footprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3_wGgcc0unAy",
   "metadata": {
    "id": "3_wGgcc0unAy"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "PwaRKa-qu3v_",
   "metadata": {
    "id": "PwaRKa-qu3v_"
   },
   "outputs": [],
   "source": [
    "def compute_model_size(model):\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "    total_size = param_size + buffer_size\n",
    "    return total_size / (1024 * 1024)  # Convert to MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "lRHEKwUDs2q_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lRHEKwUDs2q_",
    "outputId": "8c805ab4-0da3-4188-f46f-2fce86ca1f0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 23892289\n",
      "Model size (MB): 155.63\n"
     ]
    }
   ],
   "source": [
    "# Assuming your model is named 'model'\n",
    "model = EfficientVisionTransformer()\n",
    "model = torch.load('/home/emmanuel/Project/model3.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "num_params = count_parameters(model)\n",
    "print(f\"Number of parameters: {num_params}\")\n",
    "\n",
    "model_size = compute_model_size(model)\n",
    "print(f\"Model size (MB): {model_size:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VrVV6f3b_Xo5",
   "metadata": {
    "id": "VrVV6f3b_Xo5"
   },
   "source": [
    "# Model Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "JrnIuu6Jv6xc",
   "metadata": {
    "id": "JrnIuu6Jv6xc"
   },
   "outputs": [],
   "source": [
    "model = EfficientVisionTransformer()\n",
    "model = torch.load('/home/emmanuel/Project/model3.pt', map_location=torch.device('cpu'))\n",
    "\n",
    "\n",
    "\n",
    "def inverse_transform(prediction, min_values, max_values, mean, std):\n",
    "  \"\"\"\n",
    "  Applies inverse MinMaxScaler and Normalization to a prediction.\n",
    "\n",
    "  Args:\n",
    "    prediction: The predicted value (float32).\n",
    "    min_values: An array of minimum values used for scaling (float32).\n",
    "    max_values: An array of maximum values used for scaling (float32).\n",
    "    mean: An array of mean values used for normalization (float32).\n",
    "    std: An array of standard deviation values used for normalization (float32).\n",
    "\n",
    "  Returns:\n",
    "    The real-world prediction after inverse transformation (float32).\n",
    "  \"\"\"\n",
    "\n",
    "  output = (prediction - 0.5) * (max_values - min_values) + min_values\n",
    "  for i in range(output.shape[0]):\n",
    "    for j in range(output.shape[1]):\n",
    "        output[i, j] = (output[i, j] * std[j]) + mean[j]\n",
    "  return output.item() # extract single value in case of tensor\n",
    "\n",
    "\n",
    "\n",
    "def predict(image):\n",
    "    if isinstance(image, np.ndarray):\n",
    "      image = Image.fromarray(image)  # Convert to PIL image\n",
    "\n",
    "\n",
    "    # Preprocess image (assuming PyTorch and required transformations)\n",
    "    preprocessed_image = transform(image).unsqueeze(0).float()\n",
    "\n",
    "\n",
    "    # Replace min_values, max_values, mean, and std with your actual values\n",
    "    min_values = 1.8  # Array of minimum values\n",
    "    max_values = 7.1  # Array of maximum values\n",
    "    mean = [0.2498, 0.3010, 0.1964] # Mean values from normalization\n",
    "    std = [0.1668, 0.1603 , 0.1697] # Std values from normalization\n",
    "\n",
    "    # Run model inference\n",
    "    prediction = model(preprocessed_image)\n",
    "    output = inverse_transform(prediction, min_values, max_values, mean, std)\n",
    "\n",
    "    arr = np.array(output)\n",
    "    output = np.clip(arr, 0,1)\n",
    "    # Format output to 2 decimal places\n",
    "    model_output = f\"This image has a Vegetation Index of {output:.2f}\"\n",
    "\n",
    "    return model_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "YeruS1BM_f4Q",
   "metadata": {
    "id": "YeruS1BM_f4Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7883\n",
      "Running on public URL: https://f29d168563c83205cc.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f29d168563c83205cc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Gradio interface\n",
    "inputs = gr.Image(label=\"Upload image\")\n",
    "outputs = gr.Textbox(label=\"Fractional Vegetation Index\")\n",
    "\n",
    "interface = gr.Interface(fn=predict, inputs=inputs, outputs=outputs, title=\"Fractional Vegetation Index Prediction Model\")\n",
    "\n",
    "# Launch the interface\n",
    "interface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1284aa-c76e-4e1d-9c61-5af9b41cf693",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b5cee6-c989-4cc7-bf99-40aba861dcb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
