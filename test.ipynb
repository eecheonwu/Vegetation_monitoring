{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52f138ea-b422-4b6a-9897-9b1d6865309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "971661f5-7c34-4c8f-b496-aa4c1caafadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientVisionTransformer(nn.Module):\n",
    "\n",
    "  def __init__(self, d_model=600, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.2, batch_first=True):\n",
    "\n",
    "\n",
    "    super().__init__()\n",
    "    # Patch embedding\n",
    "    self.patch_embedding = nn.Conv2d(3, d_model, kernel_size=16, stride=16)\n",
    "\n",
    "    # Efficient Transformer encoder\n",
    "    encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "    self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_encoder_layers)\n",
    "\n",
    "    # Decoder\n",
    "    self.decoder = nn.Linear(d_model, 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.patch_embedding(x)\n",
    "    x = x.flatten(2).transpose(1, 2)\n",
    "    x = model.transformer_encoder(x)\n",
    "    x = self.decoder(x.mean(dim=1))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b1c0957-2a70-4688-a89b-6cc320bc5ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxScaler(nn.Module):\n",
    "  def __init__(self, min_values, max_values):\n",
    "    self.min_values = min_values\n",
    "    self.max_values = max_values\n",
    "\n",
    "  def __call__(self, data):\n",
    "    return (data - self.min_values) / (self.max_values - self.min_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37ea34af-a109-4df2-b6dd-4a6dfe53aa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(600),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.2498, 0.3010, 0.1964), (0.1668, 0.1603, 0.1697)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    MinMaxScaler(1.8, 7.1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe574433-2537-4c40-af24-9f36955249a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom dataset class\n",
    "class ImageLabelDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_file, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_file = label_file\n",
    "        self.transform = transform\n",
    "\n",
    "        # Read labels from CSV file\n",
    "        label_df = pd.read_csv(label_file)\n",
    "        self.image_paths = label_df['image'].tolist()\n",
    "        self.labels = label_df['FVC'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = os.path.join(self.image_dir, self.image_paths[index])\n",
    "        image = Image.open(image_path)\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e230bc03-3669-45dc-9617-3836d31e620d",
   "metadata": {},
   "source": [
    "# Boundary value testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68f000fd-c209-4b5c-bf79-586868cfb3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate synthetic boundary images\n",
    "def generate_boundary_images(image_size):\n",
    "    # Generate images with all pixels set to minimum and maximum values\n",
    "    min_image = torch.zeros(image_size)\n",
    "    max_image = torch.ones(image_size)\n",
    "    min_image1 = torch.full(image_size, 1.8)\n",
    "    max_image2 = torch.full(image_size, 7.1)\n",
    "\n",
    "    # Generate images with alternating pixel patterns\n",
    "    checkered_image = torch.full(image_size, 1.0)\n",
    "    checkered_image[::2, ::2] = 7.1\n",
    "    checkered_image[1::2, 1::2] = 7.1\n",
    "\n",
    "    # Add more boundary cases as needed\n",
    "    boundary_images = [min_image1, max_image2, checkered_image]\n",
    "    return boundary_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6598f020-f06f-44ae-a26b-c782a75912c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform boundary value testing\n",
    "def boundary_value_testing(model, boundary_images):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Create a dummy input tensor for the forward pass\n",
    "    dummy_input = torch.randn(1, 3, 600, 600)\n",
    "\n",
    "    # Make predictions on boundary images\n",
    "    with torch.no_grad():\n",
    "        for image in boundary_images:\n",
    "            image = image.unsqueeze(0)  # Add batch dimension\n",
    "            prediction = model(image)\n",
    "            print(f\"Boundary Case: {image.squeeze().numpy()}\")\n",
    "            print(f\"Prediction: {prediction.item()}\")\n",
    "            print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e5e61f1-bb74-492e-bb70-29271fe8cecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boundary Case: [[[1.8 1.8 1.8 ... 1.8 1.8 1.8]\n",
      "  [1.8 1.8 1.8 ... 1.8 1.8 1.8]\n",
      "  [1.8 1.8 1.8 ... 1.8 1.8 1.8]\n",
      "  ...\n",
      "  [1.8 1.8 1.8 ... 1.8 1.8 1.8]\n",
      "  [1.8 1.8 1.8 ... 1.8 1.8 1.8]\n",
      "  [1.8 1.8 1.8 ... 1.8 1.8 1.8]]\n",
      "\n",
      " [[1.8 1.8 1.8 ... 1.8 1.8 1.8]\n",
      "  [1.8 1.8 1.8 ... 1.8 1.8 1.8]\n",
      "  [1.8 1.8 1.8 ... 1.8 1.8 1.8]\n",
      "  ...\n",
      "  [1.8 1.8 1.8 ... 1.8 1.8 1.8]\n",
      "  [1.8 1.8 1.8 ... 1.8 1.8 1.8]\n",
      "  [1.8 1.8 1.8 ... 1.8 1.8 1.8]]\n",
      "\n",
      " [[1.8 1.8 1.8 ... 1.8 1.8 1.8]\n",
      "  [1.8 1.8 1.8 ... 1.8 1.8 1.8]\n",
      "  [1.8 1.8 1.8 ... 1.8 1.8 1.8]\n",
      "  ...\n",
      "  [1.8 1.8 1.8 ... 1.8 1.8 1.8]\n",
      "  [1.8 1.8 1.8 ... 1.8 1.8 1.8]\n",
      "  [1.8 1.8 1.8 ... 1.8 1.8 1.8]]]\n",
      "Prediction: -0.05743939429521561\n",
      "---\n",
      "Boundary Case: [[[7.1 7.1 7.1 ... 7.1 7.1 7.1]\n",
      "  [7.1 7.1 7.1 ... 7.1 7.1 7.1]\n",
      "  [7.1 7.1 7.1 ... 7.1 7.1 7.1]\n",
      "  ...\n",
      "  [7.1 7.1 7.1 ... 7.1 7.1 7.1]\n",
      "  [7.1 7.1 7.1 ... 7.1 7.1 7.1]\n",
      "  [7.1 7.1 7.1 ... 7.1 7.1 7.1]]\n",
      "\n",
      " [[7.1 7.1 7.1 ... 7.1 7.1 7.1]\n",
      "  [7.1 7.1 7.1 ... 7.1 7.1 7.1]\n",
      "  [7.1 7.1 7.1 ... 7.1 7.1 7.1]\n",
      "  ...\n",
      "  [7.1 7.1 7.1 ... 7.1 7.1 7.1]\n",
      "  [7.1 7.1 7.1 ... 7.1 7.1 7.1]\n",
      "  [7.1 7.1 7.1 ... 7.1 7.1 7.1]]\n",
      "\n",
      " [[7.1 7.1 7.1 ... 7.1 7.1 7.1]\n",
      "  [7.1 7.1 7.1 ... 7.1 7.1 7.1]\n",
      "  [7.1 7.1 7.1 ... 7.1 7.1 7.1]\n",
      "  ...\n",
      "  [7.1 7.1 7.1 ... 7.1 7.1 7.1]\n",
      "  [7.1 7.1 7.1 ... 7.1 7.1 7.1]\n",
      "  [7.1 7.1 7.1 ... 7.1 7.1 7.1]]]\n",
      "Prediction: -0.05737143009901047\n",
      "---\n",
      "Boundary Case: [[[7.1 7.1 7.1 ... 7.1 7.1 7.1]\n",
      "  [1.  1.  1.  ... 1.  1.  1. ]\n",
      "  [7.1 7.1 7.1 ... 7.1 7.1 7.1]\n",
      "  ...\n",
      "  [1.  1.  1.  ... 1.  1.  1. ]\n",
      "  [7.1 7.1 7.1 ... 7.1 7.1 7.1]\n",
      "  [1.  1.  1.  ... 1.  1.  1. ]]\n",
      "\n",
      " [[1.  1.  1.  ... 1.  1.  1. ]\n",
      "  [7.1 7.1 7.1 ... 7.1 7.1 7.1]\n",
      "  [1.  1.  1.  ... 1.  1.  1. ]\n",
      "  ...\n",
      "  [7.1 7.1 7.1 ... 7.1 7.1 7.1]\n",
      "  [1.  1.  1.  ... 1.  1.  1. ]\n",
      "  [7.1 7.1 7.1 ... 7.1 7.1 7.1]]\n",
      "\n",
      " [[7.1 7.1 7.1 ... 7.1 7.1 7.1]\n",
      "  [1.  1.  1.  ... 1.  1.  1. ]\n",
      "  [7.1 7.1 7.1 ... 7.1 7.1 7.1]\n",
      "  ...\n",
      "  [1.  1.  1.  ... 1.  1.  1. ]\n",
      "  [7.1 7.1 7.1 ... 7.1 7.1 7.1]\n",
      "  [1.  1.  1.  ... 1.  1.  1. ]]]\n",
      "Prediction: -0.05659707635641098\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = EfficientVisionTransformer()\n",
    "model = torch.load('/home/emmanuel/Project/model3.pt', map_location=torch.device('cpu'))\n",
    "# ... (load the model weights, e.g., model.load_state_dict(torch.load('model.pth')))\n",
    "\n",
    "# Generate boundary images\n",
    "boundary_images = generate_boundary_images((3, 600, 600))\n",
    "\n",
    "# Call the boundary value testing function\n",
    "boundary_value_testing(model, boundary_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52575f14-307a-4533-aa26-da5ff78ebfdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.13920425294637678\n"
     ]
    }
   ],
   "source": [
    "prediction = -0.05737143009901047\n",
    "output = (prediction - 0.5) * (7.0 - 1.0) + 1.0\n",
    "output = output * 0.1656 + 0.2490\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd65b238-15ea-4968-8e10-fe01e2ce58ac",
   "metadata": {},
   "source": [
    "# Regression Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b7e9f18-a69d-4d07-bdca-afc42b550f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = EfficientVisionTransformer()\n",
    "model = torch.load('/home/emmanuel/Project/model3.pt', map_location=torch.device('cpu'))\n",
    "# ... (load the model weights, e.g., model.load_state_dict(torch.load('model.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c83114e2-6eb6-4104-b293-e2e7a4c0c104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a regression test dataset\n",
    "val_dataset = ImageLabelDataset('/home/emmanuel/Project/ImageDataset/Validation', '/home/emmanuel/Project/ImageDataset/Validation/validation.csv', transform=transform)\n",
    "test_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c6944c8-b43b-4580-86c9-1b00cf25a52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform regression testing\n",
    "def regression_testing(model, test_loader, expected_results):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Iterate over the test dataset\n",
    "    with torch.no_grad():\n",
    "        for images, targets in test_loader:\n",
    "            predictions = model(images)\n",
    "            predictions = (predictions - 0.5) * (7.0 - 1.0) + 1.0\n",
    "            predictions = predictions * 0.1656 + 0.2490\n",
    "\n",
    "            # Compare predictions with expected results\n",
    "            for i in range(len(predictions)):\n",
    "                prediction = predictions[i].item()\n",
    "                expected_result = expected_results[i]\n",
    "                assert abs(prediction - expected_result) < 10e1, f\"Regression detected for input {images[i].numpy()}\"\n",
    "\n",
    "    print(\"Regression testing passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e3938bf3-9125-41c1-8d01-8105aebbeb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load expected results for the test dataset\n",
    "expected_results = [0.89,1,1,0.99,1,0.94,1,0.67,0.94,0.98,0.87,0.81,0.85,0.91,0.44,1,1,0.98,1,0.94,1,0.57,0.95,1,1,0.87,0.62,1,0.19,\n",
    "1,1,1,0.9,0.36,0.9,0.33,1,1,1,0.99,0.86,1,0.16,1,1,0.96,0.99,1,0.97,0.04,0.99,1,1,1,1,1,0.1,0.93,1,0.78,1,1,1,0.09,0.73,0.84,0.68,0.95,\n",
    "1,1,0,0.48,1,0.98,0.98,0.88,1,0,0.5,1,1,1,1,1,0.83,1,1,0.86,0.98,0.8,0.74,0.57,1,1,1,0.99,1,1,0.37,1,1,0.92,1,1,1,0.45,0.11,0.05,0.53,\n",
    "0.99,0.99,0.95,0.46,0.62,0.26,0.38,0.97,0.89,1,1,0.57,0.94,0.94,1,0.66,0.79,0.93,1,0.69,0.96,0.97,0.77,0.84,1,0.87,0.06,0.85,0.71,0.91,\n",
    "0.95,0.99,0.97,0.97,1,0.94,0.6,1,0.91,1,1,1,1,0.99,0.72,0.9,0.97,0.89,0.54,0.61,0.67,0.15,0.88,0.23,0.08,0.41,0.77,0.9,1,0.99,0.89,0.64,\n",
    "0.41,0.51,0.63,1,1,0.94,0.93,0.31,0.01,0.02,0.6,0.93,0.85,0.93,0.98,0.71,0.48,0.57,0.99,0.87,0.99,0.94,0.75,0.92,0.87,1,1,1,0.81,1,0.63,\n",
    "0.98,0.9,0.61,0.9,0.99,0.99,0.87,0.9,0.59,0.83,0.93,0.79,0.99,0.88,0.85,0.71,0.01,0.02,0.57,0.92,1,0.86,0.78,0.65,0.04,0.05,0.7,0.84,1,\n",
    "0.94,0.88,0.71,0.63,0.4,0.72,0.72,1,0.92,0.71,0.66,0.22,0,0.2,0.74,0.92,0.91,0.75,0.91,0.01,0.01,0,0,0.46,0.05,0.78,0.04,0.04,0,0.08,\n",
    "0.22,0.16,0.61,0.63,0.7,0.44,0.07,0.09,0.23,0.63,0.86,0.82,0.18,0.16,0.06,0.28,0.32,0.29,0.1,0.29,0.63,0.89]  # Replace with actual expected results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b70efef-745c-4083-807b-f72eaaa9c1bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Regression detected for input [[[-0.4580584  -0.50685394 -0.5379057  ... -0.49798203 -0.56895745\n   -0.622189  ]\n  [-0.46249434 -0.4846742  -0.4935461  ... -0.52016187 -0.55121356\n   -0.58670133]\n  [-0.4891101  -0.47136626 -0.4447505  ... -0.53346974 -0.5290338\n   -0.52016187]\n  ...\n  [-0.5822653  -0.6000092  -0.6133171  ... -0.622189   -0.6133171\n   -0.5955732 ]\n  [-0.5733934  -0.58670133 -0.60444516 ... -0.622189   -0.622189\n   -0.5822653 ]\n  [-0.5645215  -0.57782936 -0.5955732  ... -0.622189   -0.622189\n   -0.56895745]]\n\n [[-0.36618674 -0.4354243  -0.48158267 ... -0.38465008 -0.44927183\n   -0.5092777 ]\n  [-0.37541842 -0.4123451  -0.44004017 ... -0.3984976  -0.43080845\n   -0.46311936]\n  [-0.40311345 -0.40311345 -0.38926592 ... -0.4123451  -0.3938818\n   -0.3938818 ]\n  ...\n  [-0.6108261  -0.62005776 -0.6339053  ... -0.62928945 -0.62005776\n   -0.60159445]\n  [-0.60159445 -0.6154419  -0.62467355 ... -0.6385211  -0.62005776\n   -0.5877469 ]\n  [-0.5923627  -0.60621023 -0.6154419  ... -0.643137   -0.6154419\n   -0.5738994 ]]\n\n [[-0.3530606  -0.42718327 -0.50130594 ... -0.40102232 -0.475145\n   -0.54926765]\n  [-0.34870046 -0.3966622  -0.44462392 ... -0.41846296 -0.45770437\n   -0.50566614]\n  [-0.35742077 -0.37050125 -0.38794187 ... -0.42282313 -0.42718327\n   -0.44026372]\n  ...\n  [-0.557988   -0.557988   -0.557988   ... -0.557988   -0.5449075\n   -0.5274669 ]\n  [-0.5536278  -0.557988   -0.557988   ... -0.557988   -0.557988\n   -0.5143865 ]\n  [-0.5536278  -0.557988   -0.557988   ... -0.557988   -0.5536278\n   -0.50130594]]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Call the regression testing function\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m regression_testing(model, test_loader, expected_results)\n",
      "Cell \u001b[0;32mIn[30], line 17\u001b[0m, in \u001b[0;36mregression_testing\u001b[0;34m(model, test_loader, expected_results)\u001b[0m\n\u001b[1;32m     15\u001b[0m             prediction \u001b[38;5;241m=\u001b[39m predictions[i]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     16\u001b[0m             expected_result \u001b[38;5;241m=\u001b[39m expected_results[i]\n\u001b[0;32m---> 17\u001b[0m             \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(prediction \u001b[38;5;241m-\u001b[39m expected_result) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5e-02\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRegression detected for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimages[i]\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRegression testing passed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Regression detected for input [[[-0.4580584  -0.50685394 -0.5379057  ... -0.49798203 -0.56895745\n   -0.622189  ]\n  [-0.46249434 -0.4846742  -0.4935461  ... -0.52016187 -0.55121356\n   -0.58670133]\n  [-0.4891101  -0.47136626 -0.4447505  ... -0.53346974 -0.5290338\n   -0.52016187]\n  ...\n  [-0.5822653  -0.6000092  -0.6133171  ... -0.622189   -0.6133171\n   -0.5955732 ]\n  [-0.5733934  -0.58670133 -0.60444516 ... -0.622189   -0.622189\n   -0.5822653 ]\n  [-0.5645215  -0.57782936 -0.5955732  ... -0.622189   -0.622189\n   -0.56895745]]\n\n [[-0.36618674 -0.4354243  -0.48158267 ... -0.38465008 -0.44927183\n   -0.5092777 ]\n  [-0.37541842 -0.4123451  -0.44004017 ... -0.3984976  -0.43080845\n   -0.46311936]\n  [-0.40311345 -0.40311345 -0.38926592 ... -0.4123451  -0.3938818\n   -0.3938818 ]\n  ...\n  [-0.6108261  -0.62005776 -0.6339053  ... -0.62928945 -0.62005776\n   -0.60159445]\n  [-0.60159445 -0.6154419  -0.62467355 ... -0.6385211  -0.62005776\n   -0.5877469 ]\n  [-0.5923627  -0.60621023 -0.6154419  ... -0.643137   -0.6154419\n   -0.5738994 ]]\n\n [[-0.3530606  -0.42718327 -0.50130594 ... -0.40102232 -0.475145\n   -0.54926765]\n  [-0.34870046 -0.3966622  -0.44462392 ... -0.41846296 -0.45770437\n   -0.50566614]\n  [-0.35742077 -0.37050125 -0.38794187 ... -0.42282313 -0.42718327\n   -0.44026372]\n  ...\n  [-0.557988   -0.557988   -0.557988   ... -0.557988   -0.5449075\n   -0.5274669 ]\n  [-0.5536278  -0.557988   -0.557988   ... -0.557988   -0.557988\n   -0.5143865 ]\n  [-0.5536278  -0.557988   -0.557988   ... -0.557988   -0.5536278\n   -0.50130594]]]"
     ]
    }
   ],
   "source": [
    "# Call the regression testing function\n",
    "regression_testing(model, test_loader, expected_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a5c564c-bd1b-4e66-a997-71dafd5ab01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(expected_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8eb27f-71fc-4a64-b765-0f1ab7ab8339",
   "metadata": {},
   "source": [
    "# Interpretability Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fd8b37f7-e7d6-4709-af04-156ba148e20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = EfficientVisionTransformer()\n",
    "model = torch.load('/home/emmanuel/Project/model3.pt', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6811b302-92b0-40f8-892a-5512a49e001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to your sample image (replace with your actual path)\n",
    "image_path = \"/home/emmanuel/Project/ImageDataset/Test/01_00000087.jpg_tile_5_6.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf8f910a-b426-475a-9fce-5e4705157299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the image using Pillow (PIL Fork)\n",
    "img = transforms.ToTensor()(Image.open(image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "801e8e3e-17d1-420d-b996-6b4928ad1b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the image has 3 channels (RGB) for compatibility with the model\n",
    "if img.shape[0] != 3:\n",
    "    raise ValueError(\"Image must have 3 channels (RGB).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9261a3ea-d59c-4b41-9c2f-3a6f311b23f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a batch dimension (unsqueeze) for compatibility with the model\n",
    "img = img.unsqueeze(0)  # Batch size of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "982d5442-763c-47d6-b905-8aea9618ddda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image loaded and converted to tensor. Shape: torch.Size([1, 3, 600, 600])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Image loaded and converted to tensor. Shape: {img.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41ac091a-6ee2-486a-b01a-df8596a4ec9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "output = model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "239d2644-3bc7-451b-a731-13ebac1084e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic interpretability checks:\n",
    "\n",
    "# 1. Input shape compatibility\n",
    "assert img.shape[1] == 3, \"Input image must have 3 channels (RGB)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0f5bf81-31b7-4b45-9d81-421ef413eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Output shape (assuming regression for simplicity)\n",
    "assert output.shape[1] == 1, \"Model output should have 1 dimension for regression\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e69974b-ae82-49ac-8f15-f1b4b62a823a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: patch_embedding.weight, Gradient mean: -3.333434142405167e-05\n",
      "Parameter: patch_embedding.bias, Gradient mean: -0.00018543006444815546\n",
      "Parameter: transformer_encoder.layers.0.self_attn.in_proj_weight, Gradient mean: -4.6150935872901755e-07\n",
      "Parameter: transformer_encoder.layers.0.self_attn.in_proj_bias, Gradient mean: 0.00020311609841883183\n",
      "Parameter: transformer_encoder.layers.0.self_attn.out_proj.bias, Gradient mean: -1.9868215406226852e-10\n",
      "Parameter: transformer_encoder.layers.0.linear1.bias, Gradient mean: -0.0001544733386253938\n",
      "Parameter: transformer_encoder.layers.0.linear1.weight_orig, Gradient mean: 2.6261453456299932e-08\n",
      "Parameter: transformer_encoder.layers.0.linear2.bias, Gradient mean: -4.46258741765293e-12\n",
      "Parameter: transformer_encoder.layers.0.linear2.weight_orig, Gradient mean: -2.4461533243425038e-08\n",
      "Parameter: transformer_encoder.layers.0.norm1.weight, Gradient mean: -4.359465037850896e-06\n",
      "Parameter: transformer_encoder.layers.0.norm1.bias, Gradient mean: -2.3505976059823297e-05\n",
      "Parameter: transformer_encoder.layers.0.norm2.weight, Gradient mean: 2.291310465807328e-06\n",
      "Parameter: transformer_encoder.layers.0.norm2.bias, Gradient mean: -2.159691575798206e-05\n",
      "Parameter: transformer_encoder.layers.1.self_attn.in_proj_weight, Gradient mean: -1.5847255729894982e-09\n",
      "Parameter: transformer_encoder.layers.1.self_attn.in_proj_bias, Gradient mean: 8.054618774622213e-06\n",
      "Parameter: transformer_encoder.layers.1.self_attn.out_proj.bias, Gradient mean: 5.82076596124248e-13\n",
      "Parameter: transformer_encoder.layers.1.linear1.bias, Gradient mean: -7.84760004535201e-07\n",
      "Parameter: transformer_encoder.layers.1.linear1.weight_orig, Gradient mean: 4.782501505928849e-08\n",
      "Parameter: transformer_encoder.layers.1.linear2.bias, Gradient mean: 3.880510821528682e-13\n",
      "Parameter: transformer_encoder.layers.1.linear2.weight_orig, Gradient mean: 1.5945710174491978e-07\n",
      "Parameter: transformer_encoder.layers.1.norm1.weight, Gradient mean: -1.8528778582549421e-06\n",
      "Parameter: transformer_encoder.layers.1.norm1.bias, Gradient mean: -4.874029400525615e-06\n",
      "Parameter: transformer_encoder.layers.1.norm2.weight, Gradient mean: 9.151543054031208e-05\n",
      "Parameter: transformer_encoder.layers.1.norm2.bias, Gradient mean: -1.342271571047604e-05\n",
      "Parameter: transformer_encoder.layers.2.self_attn.in_proj_weight, Gradient mean: 5.677182612906506e-10\n",
      "Parameter: transformer_encoder.layers.2.self_attn.in_proj_bias, Gradient mean: -3.528709839883959e-06\n",
      "Parameter: transformer_encoder.layers.2.self_attn.out_proj.bias, Gradient mean: 1.940255410764341e-13\n",
      "Parameter: transformer_encoder.layers.2.linear1.bias, Gradient mean: 1.88646481547039e-05\n",
      "Parameter: transformer_encoder.layers.2.linear1.weight_orig, Gradient mean: -1.4525418201571938e-08\n",
      "Parameter: transformer_encoder.layers.2.linear2.bias, Gradient mean: 1.164153192248496e-12\n",
      "Parameter: transformer_encoder.layers.2.linear2.weight_orig, Gradient mean: -8.385211458517006e-08\n",
      "Parameter: transformer_encoder.layers.2.norm1.weight, Gradient mean: -7.684071192670672e-07\n",
      "Parameter: transformer_encoder.layers.2.norm1.bias, Gradient mean: 1.9633423562481767e-06\n",
      "Parameter: transformer_encoder.layers.2.norm2.weight, Gradient mean: 1.3206055200498668e-06\n",
      "Parameter: transformer_encoder.layers.2.norm2.bias, Gradient mean: -4.836334028368583e-06\n",
      "Parameter: transformer_encoder.layers.3.self_attn.in_proj_weight, Gradient mean: 4.718744145426612e-11\n",
      "Parameter: transformer_encoder.layers.3.self_attn.in_proj_bias, Gradient mean: -6.367394689732464e-06\n",
      "Parameter: transformer_encoder.layers.3.self_attn.out_proj.bias, Gradient mean: 1.746229788372744e-12\n",
      "Parameter: transformer_encoder.layers.3.linear1.bias, Gradient mean: 3.348731843288988e-05\n",
      "Parameter: transformer_encoder.layers.3.linear1.weight_orig, Gradient mean: -6.189469381467916e-09\n",
      "Parameter: transformer_encoder.layers.3.linear2.bias, Gradient mean: 1.164153192248496e-12\n",
      "Parameter: transformer_encoder.layers.3.linear2.weight_orig, Gradient mean: -2.2201143679012603e-07\n",
      "Parameter: transformer_encoder.layers.3.norm1.weight, Gradient mean: -1.3667671510120272e-06\n",
      "Parameter: transformer_encoder.layers.3.norm1.bias, Gradient mean: 9.313186666304318e-08\n",
      "Parameter: transformer_encoder.layers.3.norm2.weight, Gradient mean: 4.02254045184236e-05\n",
      "Parameter: transformer_encoder.layers.3.norm2.bias, Gradient mean: -5.783728738606442e-06\n",
      "Parameter: transformer_encoder.layers.4.self_attn.in_proj_weight, Gradient mean: 2.2761113827041157e-10\n",
      "Parameter: transformer_encoder.layers.4.self_attn.in_proj_bias, Gradient mean: -1.1187451491423417e-05\n",
      "Parameter: transformer_encoder.layers.4.self_attn.out_proj.bias, Gradient mean: -3.1044086572229457e-12\n",
      "Parameter: transformer_encoder.layers.4.linear1.bias, Gradient mean: 0.00010042781650554389\n",
      "Parameter: transformer_encoder.layers.4.linear1.weight_orig, Gradient mean: 1.9975310294739757e-07\n",
      "Parameter: transformer_encoder.layers.4.linear2.bias, Gradient mean: 7.761021643057364e-13\n",
      "Parameter: transformer_encoder.layers.4.linear2.weight_orig, Gradient mean: 4.2628633423191786e-07\n",
      "Parameter: transformer_encoder.layers.4.norm1.weight, Gradient mean: -6.5853664636961184e-06\n",
      "Parameter: transformer_encoder.layers.4.norm1.bias, Gradient mean: -3.537443990353495e-05\n",
      "Parameter: transformer_encoder.layers.4.norm2.weight, Gradient mean: -1.2234821952006314e-05\n",
      "Parameter: transformer_encoder.layers.4.norm2.bias, Gradient mean: -3.899986404576339e-05\n",
      "Parameter: transformer_encoder.layers.5.self_attn.in_proj_weight, Gradient mean: -2.3315689379188598e-09\n",
      "Parameter: transformer_encoder.layers.5.self_attn.in_proj_bias, Gradient mean: -9.18789464776637e-06\n",
      "Parameter: transformer_encoder.layers.5.self_attn.out_proj.bias, Gradient mean: 1.5522043286114728e-12\n",
      "Parameter: transformer_encoder.layers.5.linear1.bias, Gradient mean: 0.00026586322928778827\n",
      "Parameter: transformer_encoder.layers.5.linear1.weight_orig, Gradient mean: 6.678708359686425e-07\n",
      "Parameter: transformer_encoder.layers.5.linear2.bias, Gradient mean: 3.1044087439591195e-11\n",
      "Parameter: transformer_encoder.layers.5.linear2.weight_orig, Gradient mean: 1.8399969121674076e-06\n",
      "Parameter: transformer_encoder.layers.5.norm1.weight, Gradient mean: -2.237983062514104e-05\n",
      "Parameter: transformer_encoder.layers.5.norm1.bias, Gradient mean: -2.1753714463557117e-05\n",
      "Parameter: transformer_encoder.layers.5.norm2.weight, Gradient mean: 5.506371599039994e-05\n",
      "Parameter: transformer_encoder.layers.5.norm2.bias, Gradient mean: 0.0007222591084428132\n",
      "Parameter: decoder.bias, Gradient mean: 1.0\n",
      "Parameter: decoder.weight_orig, Gradient mean: -0.006296318955719471\n",
      "Interpretability checks passed (basic)!\n"
     ]
    }
   ],
   "source": [
    "# 3. Gradient check (optional, for rudimentary sanity check)\n",
    "output.backward()  # Calculate gradients\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        print(f\"Parameter: {name}, Gradient mean: {param.grad.mean().item()}\")\n",
    "print(\"Interpretability checks passed (basic)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba19e821-1269-4980-97d6-9589bfcf5032",
   "metadata": {},
   "source": [
    "## Interpretabilty Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c47001cd-752d-4ae8-8e31-a0e0e9a4b93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(prediction):\n",
    "    output = (prediction - 0.5) * (7.0 - 1.0) + 1.0\n",
    "    output = output * 0.1656 + 0.2490\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "633d4593-9a59-4d48-b7d3-0988c2d429a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass to get output probabilities\n",
    "# Define the path to your sample image (replace with your actual path)\n",
    "image_path = \"/home/emmanuel/Project/ImageDataset/Test/01_00000087.jpg_tile_5_6.jpg\"\n",
    "img = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb205923-ba57-4889-bc70-b04b277bc9db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fb8c973e-c1c2-42c1-8225-14916ee72d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    # Assuming image is a tensor with shape (channels, height, width)\n",
    "    mean = torch.tensor([0.2498, 0.3010, 0.1964])  # Replace with appropriate mean values if needed\n",
    "    std = torch.tensor([0.1668, 0.1603, 0.1697])  # Replace with appropriate std values if needed\n",
    "    image = image.permute(1, 2, 0)  # Move channel dimension to last if necessary\n",
    "    image = (image - mean) / std  # Normalize based on mean and standard deviation\n",
    "    return image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d73b273-4fd3-4630-9d15-28a5fd55ffd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "359a1f71-2b63-4a64-8da5-d9b0be10dd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_images(original_image, perturbed_image, title=\"Original vs. Perturbed Image\"):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "    # Assuming original_image and perturbed_image are preprocessed tensors\n",
    "    #original_image = original_image.squeeze()  # Move channel dimension to last\n",
    "    #perturbed_image = perturbed_image.squeeze()\n",
    "\n",
    "    original_image = original_image.squeeze() * 255  # Assuming uint8 image\n",
    "    perturbed_image = perturbed_image.squeeze()* 255  # Assuming uint8 image\n",
    "    original_image = torch.clamp(original_image, 0, 255)  # Clip values to 0-255\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 255)  # Clip values to 0-255\n",
    "    original_image = original_image.to(torch.uint8)  # Convert to uint8 for proper display\n",
    "    perturbed_image = perturbed_image.to(torch.uint8)  # Convert to uint8 for proper display\n",
    "\n",
    "    ax1.imshow(original_image)\n",
    "    ax1.set_title(\"Original Image\")\n",
    "    ax1.axis('off')\n",
    "\n",
    "    ax2.imshow(perturbed_image)\n",
    "    ax2.set_title(\"Perturbed Image\")\n",
    "    ax2.axis('off')\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d633462-3972-4b52-bcbb-3d10ef4febd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define perturbation function (replace with your desired modifications)\n",
    "def perturb(image, mask_size=32):\n",
    "    mask = torch.zeros_like(image[0])\n",
    "    mask.random_(0, 255)\n",
    "    mask[:, mask_size // 2:-mask_size // 2, mask_size // 2:-mask_size // 2] = 255  # Apply mask to a central region\n",
    "    return image + mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dca742ce-4d7b-4172-9353-e0c09ea71650",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "820a36d5-1460-4950-a0c1-ca6d91af5854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the image for perturbation\n",
    "perturbed_image = img.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "37a053e6-c940-46da-9f94-c9be1cf19868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perturb the image and get predictions\n",
    "perturbed_output = model(perturb(perturbed_image.unsqueeze(0)))\n",
    "original_prediction = model(img.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2fdc13a7-a79c-43c2-ac0a-3e4fb4e8f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract prediction values from tensors (assuming they're single-valued)\n",
    "original_prediction = original_prediction.item()\n",
    "perturbed_prediction = perturbed_output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ede8952b-e6fc-4aea-ae7d-8c5a1e315dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess images for visualization (adjust based on your preprocessing steps)\n",
    "original_image_vis = preprocess_image(img.clone())\n",
    "perturbed_image_vis = preprocess_image(perturbed_image.clone())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "beedfbae-9f73-473e-848c-6f42b42d80aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the original image and the perturbed image with formatted predictions\n",
    "visualize_images(original_image_vis, perturbed_image_vis, title=f\"Original Prediction: {original_prediction:.4f}, Perturbed Prediction: {perturbed_prediction:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0d4cefde-8e1e-4a5c-b75b-38a75bb6eb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_original = predict(original_prediction)\n",
    "transformed_perturbed = predict(perturbed_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ad0c405b-e719-4ca6-b7fa-f2d6ca95da8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.974510347366333 -0.14501840672492983\n"
     ]
    }
   ],
   "source": [
    "print(transformed_original, transformed_perturbed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a5df8-ee46-46f8-a20d-cf5cd84076eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d600c72c-d5b3-463c-9e23-41ea3d414631",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
